from fastapi import APIRouter, HTTPException
from app.models.chat import ChatRequest, ChatResponse, Source
from app.services.opensearch_client import opensearch_client
from app.services.bedrock_client import bedrock_client
from typing import List, Tuple
import os
import re
import time
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

router = APIRouter()

def calculate_priority_score(keyword: str, category: str) -> int:
    """æ”¹è‰¯ç‰ˆï¼šå®Œå…¨ãªä¼æ¥­åã‚’æœ€å„ªå…ˆã«ã™ã‚‹å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = 0
    
    # å®Œå…¨ãªä¼æ¥­åãƒ»çµ„ç¹”åãŒæœ€é«˜å„ªå…ˆåº¦
    if 'ä¼æ¥­å' in category and 'ã®ä¸€éƒ¨' not in category:
        score += 1000
    elif 'çµ„ç¹”å' in category and 'ã®ä¸€éƒ¨' not in category:
        score += 950
    # ä¼æ¥­åãƒ»çµ„ç¹”åã®ä¸€éƒ¨ã¯ä¸­ç¨‹åº¦å„ªå…ˆåº¦
    elif 'ä¼æ¥­åã®ä¸€éƒ¨' in category:
        score += 700
    elif 'çµ„ç¹”åã®ä¸€éƒ¨' in category:
        score += 650
    # ãã®ä»–ã®å›ºæœ‰åè©
    elif 'å›ºæœ‰åè©' in category:
        score += 800
    
    # ã‚µãƒ¼ãƒ“ã‚¹åã€è£½å“åã€ã‚²ãƒ¼ãƒ åï¼ˆå®Œå…¨å½¢ã‚’å„ªå…ˆï¼‰
    if any(x in category for x in ['ã‚µãƒ¼ãƒ“ã‚¹å', 'ã‚µãƒ¼ãƒ“ã‚¹']) and 'ã®ä¸€éƒ¨' not in category:
        score += 900
    elif any(x in category for x in ['è£½å“å', 'è£½å“']) and 'ã®ä¸€éƒ¨' not in category:
        score += 900
    elif any(x in category for x in ['ã‚²ãƒ¼ãƒ å', 'ã‚²ãƒ¼ãƒ ']) and 'ã®ä¸€éƒ¨' not in category:
        score += 900
    # éƒ¨åˆ†çš„ãªã‚µãƒ¼ãƒ“ã‚¹åãƒ»è£½å“å
    elif any(x in category for x in ['ã‚µãƒ¼ãƒ“ã‚¹', 'è£½å“', 'ã‚²ãƒ¼ãƒ ']) and 'ã®ä¸€éƒ¨' in category:
        score += 600
    
    # æŠ€è¡“ç”¨èª
    if 'æŠ€è¡“' in category:
        score += 500
    
    # å°‚é–€ç”¨èª
    if 'å°‚é–€' in category:
        score += 400
    
    # ä¸€èˆ¬åè©ã¯ä½å„ªå…ˆåº¦
    if category == 'åè©':
        score += 200
    
    # æ•°è©ã¯ä¸­ç¨‹åº¦
    if 'æ•°è©' in category:
        score += 300
    
    # æ–‡å­—æ•°ã«ã‚ˆã‚‹å¾®èª¿æ•´ï¼ˆé•·ã„ã»ã©å…·ä½“çš„ï¼‰
    score += len(keyword) * 5
    
    # ç‰¹å®šã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒœãƒ¼ãƒŠã‚¹ï¼ˆå®Œå…¨ä¸€è‡´ã®ã¿ï¼‰
    important_terms = [
        'ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—', 'ã‚«ãƒ—ã‚³ãƒ³', 'ãƒªã‚³ãƒ¼', 'atama plus',
        'Amazon', 'AWS', 'Bedrock', 'Claude',
        'ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼ãƒãƒ³ã‚¿ãƒ¼ãƒ¯ã‚¤ãƒ«ã‚º', 'ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼ãƒãƒ³ã‚¿ãƒ¼',
        'Agentic AI', 'ç”ŸæˆAI'
    ]
    
    if keyword in important_terms:
        score += 100
    
    return score

async def search_with_score_based_fallback(query: str, keywords_with_scores: list) -> Tuple[list, str, float]:
    """ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Ÿè¡Œæ™‚é–“æ¸¬å®šä»˜ãï¼‰"""
    opensearch_start = time.time()
    
    if not keywords_with_scores:
        print("âš ï¸ No keywords available, using original query")
        results = await opensearch_client.search_with_transcript_content("aws_summit_sessions", query, 3)
        opensearch_time = time.time() - opensearch_start
        return results, query, opensearch_time
    
    # è©¦è¡Œã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æº–å‚™
    search_candidates = []
    
    # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰
    for keyword_info in keywords_with_scores:
        keyword = keyword_info['keyword']
        score = keyword_info['priority']
        
        search_candidates.append({
            'keyword': keyword,
            'score': score,
            'reason': 'primary' if score < 1000 else 'high_specificity'
        })
    
    # é«˜ã‚¹ã‚³ã‚¢ï¼ˆâ‰¥1000ï¼‰ã®å ´åˆã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œã«è¿½åŠ 
    high_score_keywords = [k for k in keywords_with_scores if k['priority'] >= 1000]
    if high_score_keywords:
        print(f"ğŸ”„ High specificity keywords detected: {[k['keyword'] for k in high_score_keywords]}")
        
        # ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œã«è¿½åŠ 
        for keyword_info in keywords_with_scores:
            if 500 <= keyword_info['priority'] < 1000:
                search_candidates.append({
                    'keyword': keyword_info['keyword'],
                    'score': keyword_info['priority'],
                    'reason': 'fallback_general'
                })
    
    # é‡è¤‡é™¤å»ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é †åºã‚’ä¿æŒï¼‰
    seen_keywords = set()
    unique_candidates = []
    for candidate in search_candidates:
        if candidate['keyword'] not in seen_keywords:
            unique_candidates.append(candidate)
            seen_keywords.add(candidate['keyword'])
    
    print(f"ğŸ¯ Search strategy: {len(unique_candidates)} candidates")
    for i, candidate in enumerate(unique_candidates):
        print(f"   {i+1}. '{candidate['keyword']}' (score: {candidate['score']}, reason: {candidate['reason']})")
    
    # é †æ¬¡æ¤œç´¢å®Ÿè¡Œï¼ˆéæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
    for i, candidate in enumerate(unique_candidates):
        keyword = candidate['keyword']
        reason = candidate['reason']
        
        print(f"ğŸ” Search attempt {i+1}: '{keyword}' (score: {candidate['score']}, reason: {reason})")
        
        results = await opensearch_client.search_with_transcript_content("aws_summit_sessions", keyword, 3)
        
        if results and len(results) > 0:
            opensearch_time = time.time() - opensearch_start
            print(f"âœ… Success with '{keyword}' - Found {len(results)} results (OpenSearch time: {opensearch_time:.3f}s)")
            return results, keyword, opensearch_time
        else:
            print(f"âŒ No results with '{keyword}'")
    
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆéæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
    print(f"ğŸ†˜ Final fallback with original query: '{query}'")
    final_results = await opensearch_client.search_with_transcript_content("aws_summit_sessions", query, 3)
    opensearch_time = time.time() - opensearch_start
    return final_results, query, opensearch_time

def parse_and_prioritize_keywords_advanced(llm_result: str) -> list:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’è©³ç´°ã«ä¿æŒã™ã‚‹ç‰ˆ"""
    keywords = []
    lines = llm_result.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or any(skip in line for skip in ['è§£æçµæœ', 'è³ªå•ã®', 'ä»¥ä¸‹ã®']):
            continue
            
        import re
        pattern = r'^([^(ï¼ˆ]+)[ï¼ˆ(]([^)ï¼‰]+)[ï¼‰)]'
        match = re.match(pattern, line)
        
        if match:
            keyword = match.group(1).strip()
            category = match.group(2).strip()
            
            if len(keyword) > 1 and keyword not in ['ã«ã¤ã„ã¦', 'ã‚’', 'ã®', 'ãŒ', 'ã¯']:
                priority_score = calculate_priority_score(keyword, category)
                keywords.append({
                    'keyword': keyword,
                    'category': category,
                    'priority': priority_score,
                    'length': len(keyword)
                })
                print(f"âœ… Keyword: '{keyword}', Category: '{category}', Priority: {priority_score}")
    
    if not keywords:
        print("âš ï¸ No keywords extracted from advanced parsing")
        return []
    
    # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
    keywords.sort(key=lambda x: (x['priority'], x['length']), reverse=True)
    
    print(f"ğŸ”„ Advanced prioritized keywords:")
    for i, k in enumerate(keywords):
        fallback_indicator = " (FALLBACK CANDIDATE)" if k['priority'] >= 1000 else ""
        print(f"   {i+1}. '{k['keyword']}' (priority: {k['priority']}, category: {k['category']}){fallback_indicator}")
    
    return keywords

async def extract_search_keywords_with_llm(query: str) -> Tuple[str, float]:
    """ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œç‰ˆï¼ˆLLMå®Ÿè¡Œæ™‚é–“æ¸¬å®šä»˜ãï¼‰"""
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¯å¾“æ¥é€šã‚Š
    session_id_match = re.search(r'[A-Z]+-\d+', query)
    if session_id_match:
        return session_id_match.group(), 0.0
    
    llm_keyword_start = time.time()
    
    extraction_prompt = f"""ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å½¢æ…‹ç´ è§£æã—ã¦ã€æ¤œç´¢ã«æœ‰ç”¨ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

ãƒ«ãƒ¼ãƒ«:
- ä¼æ¥­åã€çµ„ç¹”åã€ã‚µãƒ¼ãƒ“ã‚¹åã€è£½å“åãªã©ã®å›ºæœ‰åè©ã¯ã€å®Œå…¨å½¢ã¨æ§‹æˆè¦ç´ ã®ä¸¡æ–¹ã‚’å‡ºåŠ›ã™ã‚‹
- å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å“è©ã¨è©³ç´°ãªåˆ†é¡ã‚’ä»˜ä¸ã™ã‚‹
- åŠ©è©ã€å‹•è©ã€å½¢å®¹è©ã¯é™¤å¤–ã™ã‚‹
- ä»¥ä¸‹ã®å½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹ï¼š

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(å“è©ãƒ»åˆ†é¡)

ä¾‹:
å…¥åŠ›: "ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã®å–ã‚Šçµ„ã¿ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"
å‡ºåŠ›: 
ã‚½ãƒ‹ãƒ¼(å›ºæœ‰åè©ãƒ»ä¼æ¥­åã®ä¸€éƒ¨)
ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—(å›ºæœ‰åè©ãƒ»ä¼æ¥­å)
ã‚°ãƒ«ãƒ¼ãƒ—(åè©)
å–ã‚Šçµ„ã¿(åè©)

è§£æçµæœ:"""

    try:
        llm_result = await bedrock_client.generate_guarded_response(extraction_prompt)
        llm_keyword_time = time.time() - llm_keyword_start
        print(f"ğŸ§  LLM keyword extraction completed in {llm_keyword_time:.3f}s")
        print(f"ğŸ§  LLM analysis result:\n{llm_result}")
        
        # è©³ç´°ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—
        keywords_with_scores = parse_and_prioritize_keywords_advanced(llm_result)
        
        if keywords_with_scores:
            # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆæ™‚é–“æ¸¬å®šã¯å†…éƒ¨ã§å®Ÿè¡Œæ¸ˆã¿ï¼‰
            search_results, selected_keyword, opensearch_time = await search_with_score_based_fallback(
                query, keywords_with_scores
            )
            
            total_llm_time = llm_keyword_time  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºæ™‚é–“ã®ã¿
            print(f"ğŸ¯ Selected keyword after fallback: '{selected_keyword}' (LLM keyword time: {total_llm_time:.3f}s)")
            return selected_keyword, total_llm_time
        
        print("âš ï¸ No keywords extracted, using original query")
        return query, llm_keyword_time
        
    except Exception as e:
        llm_keyword_time = time.time() - llm_keyword_start
        print(f"âŒ LLM extraction failed: {e} (time: {llm_keyword_time:.3f}s)")
        return query, llm_keyword_time

@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    # å…¨ä½“å‡¦ç†æ™‚é–“ã®æ¸¬å®šé–‹å§‹
    total_start = time.time()
    
    try:
        message = request.message.strip()
        
        if not message:
            raise HTTPException(status_code=400, detail="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™")
        
        print(f"ğŸ’¬ User query: \"{message}\"")
        
        # LLMã‚’ä½¿ã£ãŸé«˜åº¦ãªæ§‹é€ åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå®Ÿè¡Œæ™‚é–“æ¸¬å®šä»˜ãï¼‰
        search_query, llm_keyword_time = await extract_search_keywords_with_llm(message)
        print(f"ğŸ” Final search query: \"{search_query}\"")
        
        # OpenSearchæ¤œç´¢ã®å®Ÿè¡Œæ™‚é–“æ¸¬å®š
        opensearch_start = time.time()
        search_results = await opensearch_client.search_with_transcript_content(
            "aws_summit_sessions",
            search_query,
            3
        )
        opensearch_time = time.time() - opensearch_start
        
        print(f"ğŸ“Š Found {len(search_results)} relevant documents (OpenSearch time: {opensearch_time:.3f}s)")
        
        # transcript_summaryãŒå«ã¾ã‚Œã‚‹çµæœã®ç¢ºèª
        transcript_count = sum(1 for result in search_results if result.get('has_transcript'))
        if transcript_count > 0:
            print(f"ğŸ“„ Including {transcript_count} results with detailed transcript content")
        
        # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = ""
        sources = []
        
        if search_results:
            context = "ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š\n\n"
            
            for index, result in enumerate(search_results):
                context += f"ã€å‚è€ƒè³‡æ–™{index + 1}ã€‘\n"
                context += f"ã‚¿ã‚¤ãƒˆãƒ«: {result['source']['title']}\n"
                
                # AWS Summitç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
                if 'abstract' in result['source']:
                    context += f"æ¦‚è¦: {result['source']['abstract']}\n"
                
                # è©³ç´°ãªè¬›æ¼”è¦ç´„ãŒã‚ã‚‹å ´åˆã¯å„ªå…ˆçš„ã«ä½¿ç”¨
                if 'transcript_summary' in result['source'] and result['source']['transcript_summary']:
                    context += f"è©³ç´°å†…å®¹: {result['source']['transcript_summary']}\n"
                    print(f"ğŸ“„ Added transcript summary for: {result['source']['title']}")
                
                if 'speakers' in result['source'] and result['source']['speakers']:
                    speaker_names = []
                    for speaker in result['source']['speakers']:
                        speaker_names.append(f"{speaker['name']}ï¼ˆ{speaker['company']}ï¼‰")
                    context += f"è¬›æ¼”è€…: {', '.join(speaker_names)}\n"
                
                if 'track' in result['source']:
                    context += f"ãƒˆãƒ©ãƒƒã‚¯: {result['source']['track']}\n"
                
                if 'date' in result['source'] and 'start_time' in result['source']:
                    context += f"é–‹å‚¬æ—¥æ™‚: {result['source']['date']} {result['source']['start_time']}\n"
                
                context += "\n"
                
                # transcriptæœ‰ç„¡ã®æƒ…å ±ã‚’Sourceã«è¿½åŠ 
                source_title = result['source']['title']
                if result.get('has_transcript'):
                    source_title += " [è©³ç´°å†…å®¹ã‚ã‚Š]"
                
                sources.append(Source(
                    title=source_title,
                    score=f"{result['score']:.4f}"
                ))
        else:
            context = "é–¢é€£ã™ã‚‹å‚è€ƒè³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n"
        
        # 3. LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = f"""{context}

è³ªå•: {message}

ä»¥ä¸‹ã®ç‚¹ã‚’å®ˆã£ã¦æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
- å¿…ãšæ—¥æœ¬èªã§å›ç­”ã™ã‚‹
- ä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã‚’ä½¿ã†
- å‚è€ƒè³‡æ–™ãŒã‚ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚’åŸºã«å›ç­”ã™ã‚‹
- è©³ç´°å†…å®¹ãŒã‚ã‚‹å ´åˆã¯ã€ãã®æƒ…å ±ã‚’ç©æ¥µçš„ã«æ´»ç”¨ã™ã‚‹
- å‚è€ƒè³‡æ–™ãŒãªã„å ´åˆã¯ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§å›ç­”ã™ã‚‹

å›ç­”:"""
        
        print(f"ğŸ¤– Generating response with context from {len(search_results)} sources")
        
        # 4. LLMå›ç­”ç”Ÿæˆã®å®Ÿè¡Œæ™‚é–“æ¸¬å®š
        llm_response_start = time.time()
        llm_result = await bedrock_client.generate_guarded_response(prompt)
        llm_response_time = time.time() - llm_response_start
        
        print(f"ğŸ¤– LLM response generated in {llm_response_time:.3f}s")
        
        # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ­£è¦åŒ–
        final_response = llm_result.strip() if isinstance(llm_result, str) else str(llm_result)
        
        # å…¨ä½“å‡¦ç†æ™‚é–“ã®è¨ˆç®—
        total_time = time.time() - total_start
        
        # åˆè¨ˆLLMæ™‚é–“ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º + å›ç­”ç”Ÿæˆï¼‰
        total_llm_time = llm_keyword_time + llm_response_time
        
        print(f"â±ï¸ Performance Summary:")
        print(f"   - OpenSearch time: {opensearch_time:.3f}s")
        print(f"   - LLM total time: {total_llm_time:.3f}s (keyword: {llm_keyword_time:.3f}s + response: {llm_response_time:.3f}s)")
        print(f"   - Total time: {total_time:.3f}s")
        
        return ChatResponse(
            success=True,
            response=final_response,
            sources=sources,
            context_used=len(search_results) > 0,
            debug={
                "search_results_count": len(search_results),
                "transcript_results_count": transcript_count,
                "guardrail_applied": False,
                "original_query": message,
                "optimized_query": search_query,
                "search_method": "hybrid_with_transcript",
                "performance": {
                    "opensearch_time": round(opensearch_time, 3),
                    "llm_time": round(total_llm_time, 3),
                    "llm_keyword_time": round(llm_keyword_time, 3),
                    "llm_response_time": round(llm_response_time, 3),
                    "total_time": round(total_time, 3)
                }
            }
        )
        
    except Exception as error:
        total_time = time.time() - total_start
        print(f"âŒ Chat API error: {error} (total time: {total_time:.3f}s)")
        raise HTTPException(
            status_code=500,
            detail=str(error)
        )
